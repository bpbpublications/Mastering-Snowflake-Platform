/* Practical: Data loading and unloading
Creating stages
Exercise 1:
•	Create a Named internal stage to store files in CSV format. Use the PUT command to load data from your local to the internal stage.
•	List files present in the stage created in the above step using the LIST command.
Exercise 2:
•	Create a table and locate its stage 
•	Use the PUT command to load data from your local to the table stage.
•	List files present in the table stage  
Exercise 3:
•	Create a user and locate its stage 
•	Use the PUT command to load data from your local to the user stage
•	List files present in the table stage created 

Creating file formats
Exercise 4:
•	Create a file format with the following properties:
o	Fixed width file format delimited by ~ (tilde)
o	Contains header record
o	Replace string nulls with NULL
•	Create a file format with the following properties:
o	File is Parquet format file
o	Files are Snappy compressed 
•	List the file formats present using SHOW FILE FORMATS
•	Create the same file formats with different names using Web UI

Creating storage integrations
Exercise 5:
•	Create a AWS storage integration and use to load files:
o	Setup a demo account on AWS if you don’t have any 
o	Create a storage integration using CREATE command
o	List the integration created  
•	Load source files from storage integrations to target table
o	Create a sample table at your snowflake database and schema
o	Set the corresponding source files to AWS location
o	Create a storage integration to read files from AWS ( or use the one created in above task)
o	Use COPY INTO to load files from storage integration 
Creating LOAD commands and load using COPY
Exercise 6: 
•	Load files into the target table with these load conditions:
o	Load fails even if a single error occurred.
o	Loads a delimited CSV file with pipe delimiter (|).
o	Ignore the null records, if any.
•	Load files into the target table with these load conditions:
o	Load fails when 10% of total records are errored out.
o	Loads a delimited CSV file with comma delimiter (,).
o	Skip the file metadata check.
Load from local using COPY
Exercise 7:
•	Load files from local into the target table with these load conditions:
o	Load fails even if single error occurred.
o	Loads a delimited CSV file with pipe delimiter (|).
•	Load files from local using PUT and COPY command into the target table with these load conditions:
o	Upload sample files from local to internal stage.
o	Load files from internal stage to target table.
Unload from local using COPY
Exercise 8: 
•	Unload Snowflake table into Named internal stage with these unload conditions:
o	Unload into SINGLE file 
o	Unloads a delimited CSV file with pipe delimiter (|).
•	Unload Snowflake table into Named internal stage and download using GET command with these unload conditions:
o	Unload into multiple files.
o	Unloads a delimited CSV file with pipe delimiter (|). */

/* Practical: Data loading and unloading
Creating stages
Exercise 1:
•	Create a Named internal stage to store files in CSV format. Use the PUT command to load data from your local to the internal stage.
•	List files present in the stage created in the above step using the LIST command.
Exercise 2:
•	Create a table and locate its stage 
•	Use the PUT command to load data from your local to the table stage.
•	List files present in the table stage  
Exercise 3:
•	Create a user and locate its stage 
•	Use the PUT command to load data from your local to the user stage
•	List files present in the table stage created 

Creating file formats
Exercise 4:
•	Create a file format with the following properties:
o	Fixed width file format delimited by ~ (tilde)
o	Contains header record
o	Replace string nulls with NULL
•	Create a file format with the following properties:
o	File is Parquet format file
o	Files are Snappy compressed 
•	List the file formats present using SHOW FILE FORMATS
•	Create the same file formats with different names using Web UI

Creating storage integrations
Exercise 5:
•	Create a AWS storage integration and use to load files:
o	Setup a demo account on AWS if you don’t have any 
o	Create a storage integration using CREATE command
o	List the integration created  
•	Load source files from storage integrations to target table
o	Create a sample table at your snowflake database and schema
o	Set the corresponding source files to AWS location
o	Create a storage integration to read files from AWS ( or use the one created in above task)
o	Use COPY INTO to load files from storage integration 
Creating LOAD commands and load using COPY
Exercise 6: 
•	Load files into the target table with these load conditions:
o	Load fails even if a single error occurred.
o	Loads a delimited CSV file with pipe delimiter (|).
o	Ignore the null records, if any.
•	Load files into the target table with these load conditions:
o	Load fails when 10% of total records are errored out.
o	Loads a delimited CSV file with comma delimiter (,).
o	Skip the file metadata check.
Load from local using COPY
Exercise 7:
•	Load files from local into the target table with these load conditions:
o	Load fails even if single error occurred.
o	Loads a delimited CSV file with pipe delimiter (|).
•	Load files from local using PUT and COPY command into the target table with these load conditions:
o	Upload sample files from local to internal stage.
o	Load files from internal stage to target table.
Unload from local using COPY
Exercise 8: 
•	Unload Snowflake table into Named internal stage with these unload conditions:
o	Unload into SINGLE file 
o	Unloads a delimited CSV file with pipe delimiter (|).
•	Unload Snowflake table into Named internal stage and download using GET command with these unload conditions:
o	Unload into multiple files.
o	Unloads a delimited CSV file with pipe delimiter (|). */

use role accountadmin;

use database demo_db;

use schema poc;

/* Exercise 1:
•	Create a Named internal stage to store files in CSV format. Use the PUT command to load data from your local to the internal stage.
•	List files present in the stage created in the above step using the LIST command.
*/


CREATE OR REPLACE FILE FORMAT my_csv_format
  TYPE = CSV
  FIELD_DELIMITER = ','
  SKIP_HEADER = 1
  NULL_IF = ('NULL', 'null')
  EMPTY_FIELD_AS_NULL = true;
  
CREATE STAGE csv_int_stage
  FILE_FORMAT = my_csv_format;

/* run PUT from snowsql CLI or use snowsight stage upload feature */

PUT file:///tmp/data/mydata.csv @csv_int_stage; --change the file path point to your local directory and file. you can create sample csv file to test

LIST @csv_int_stage/mydata.csv;  --change the file name to your file or use LIST to list down all files from stage




/* Exercise 2:
•	Create a table and locate its stage 
•	Use the PUT command to load data from your local to the table stage.
•	List files present in the table stage  */

/* create table*/
create table cust_info (cust_id integer, cust_name string, cust_email string, addr string);

/* list stage */

LIST @%cust_info;

/* upload file to table stage */

PUT file:///tmp/data/cust_info.csv @%cust_info; --change the file path point to your local directory and file. you can create sample csv file to test

-- use data from this sql command -- insert into customer_details values(10120,'smith','smith.c@gmail.com','north york');

/* list stage */

LIST @%cust_info;


/* Exercise 3:
•	Create a user and locate its stage 
•	Use the PUT command to load data from your local to the user stage
•	List files present in the table stage created  */

/* Create User */

CREATE USER ds_engineer 
PASSWORD='abc@1234' 
MUST_CHANGE_PASSWORD = FALSE;

/* logon with user and use current user list command to get files or use userstage to list */

/* locate current user stage*/

LS @~;

/* upload file to table stage */

PUT file:///tmp/data/cust_info.csv @%cust_info; --change the file path point to your local directory and file. you can create sample csv file to test

/* logon with user and use current user list command to get files or use userstage to list */

/* locate current user stage*/

LS @~;

/* Creating file formats
Exercise 4:
•	Create a file format with the following properties:
o	Fixed width file format delimited by ~ (tilde)
o	Contains header record
o	Replace string nulls with NULL */



CREATE OR REPLACE FILE FORMAT delimited_file_format
  TYPE = CSV
  FIELD_DELIMITER = '~'
  SKIP_HEADER = 1
  NULL_IF = ('NULL', 'null')
  EMPTY_FIELD_AS_NULL = true;
  
/* •	Create a file format with the following properties:
o	File is Parquet format file
o	Files are Snappy compressed 
•	List the file formats present using SHOW FILE FORMATS
•	Create the same file formats with different names using Web UI */

CREATE OR REPLACE FILE FORMAT parquet_file_format
  TYPE = PARQUET
  COMPRESSION = SNAPPY;

/* Creating storage integrations
Exercise 5:
•	Create a AWS storage integration and use to load files:
o	Setup a demo account on AWS if you don’t have any 
o	Create a storage integration using CREATE command
o	List the integration created  
•	Load source files from storage integrations to target table
o	Create a sample table at your snowflake database and schema
o	Set the corresponding source files to AWS location
o	Create a storage integration to read files from AWS ( or use the one created in above task)
o	Use COPY INTO to load files from storage integration 
Creating LOAD commands and load using COPY */

--ensure you have AWS account, role created before you proceed with external stage creation for S3

/* Create storage integration with AWS S3 */

CREATE STORAGE INTEGRATION aws_int
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'S3'
  STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::001234567890:role/myrole'  --give your AWS role ARN here 
  ENABLED = TRUE
  STORAGE_ALLOWED_LOCATIONS = ('s3://load/files/'); --update this path to your one or more S3 bucket locations
  

/* create external stage */

CREATE STAGE my_ext_stage
  URL='s3://load/files/'  --update this s3 location from where you want to load data. this is S3 path where your load files are stored
  STORAGE_INTEGRATION = aws_int
  FILE_FORMAT = my_csv_format;


/* list files */

LIST @my_ext_stage;

/* create sample load files for cust_info table created in above step */
/* use data values from below data records - 
10120,'smith','smith.c@gmail.com','north york'
11230,'joe','joems@gmail.com','NJ'

--upload the CSV created to the AWS S3 as per storage integration and external stage 

/* COPY to load data */

COPY INTO cust_info
  FROM @my_ext_stage/cust_info.csv;

/* use this command to specify the format */

COPY INTO cust_info
  FROM @my_ext_stage
  PATTERN='cust_info*.csv';

  
/*Exercise 6: 
•	Load files into the target table with these load conditions:
o	Load fails even if a single error occurred.
o	Loads a delimited CSV file with pipe delimiter (|).
o	Ignore the null records, if any. */

--use above integration and files uploaded to external stage
--COPY to be updated to include these conditions


CREATE OR REPLACE FILE FORMAT pipe_delimited_format
  TYPE = CSV
  FIELD_DELIMITER = '|'
  SKIP_HEADER = 1
  NULL_IF = ('NULL', 'null')
  EMPTY_FIELD_AS_NULL = true;

CREATE STAGE pipe_ext_stage
  URL='s3://load/files/'  --update this s3 location from where you want to load data. this is S3 path where your load files are stored
  STORAGE_INTEGRATION = aws_int
  FILE_FORMAT = pipe_delimited_format;

/* create sample load files for cust_info table created in above step */
/* use data values from below inserts and pipe delimited 
10120|'smith'|'smith.c@gmail.com'|'north york'
11230|'joe'|'joems@gmail.com'|'NJ'
110111|'smith c'|'smith.c@gmail.com'|'ON' */

--upload the file as cust_load.csv  to the AWS S3 as per storage integration and external stage 

COPY INTO cust_info
  FROM @pipe_ext_stage
  PATTERN='cust_load*.csv';
  

/*	Load files into the target table with these load conditions:
o	Load fails when 10% of total records are errored out.
o	Loads a delimited CSV file with comma delimiter (,).
o	Skip the file metadata check.*/


--use above integration and files uploaded to external stage
--COPY to be updated to include these conditions

COPY INTO cust_info
  FROM @my_ext_stage
  PATTERN='cust_info*.csv'
  ON_ERROR='SKIP_FILE_10%'
  FORCE = TRUE;
  

/*Load from local using COPY
Exercise 7:
•	Load files from local into the target table with these load conditions:
o	Load fails even if single error occurred.
o	Loads a delimited CSV file with pipe delimiter (|).
•	Load files from local using PUT and COPY command into the target table with these load conditions:
o	Upload sample files from local to internal stage.
o	Load files from internal stage to target table. */

-- use the internal stage created in the first 2 exercises and files uploaded using PUT
--use below COPY to load

COPY INTO cust_info
  FROM @csv_int_stage;

/*Unload from local using COPY
Exercise 8: 
•	Unload Snowflake table into Named internal stage with these unload conditions:
o	Unload into SINGLE file 
o	Unloads a delimited CSV file with pipe delimiter (|) */


CREATE STAGE unload_stage
  FILE_FORMAT = pipe_delimited_format;
  
COPY INTO @unload_stage 
from cust_info 
SINGLE = TRUE;

/*•	Unload Snowflake table into Named internal stage and download using GET command with these unload conditions:
o	Unload into multiple files.
o	Unloads a delimited CSV file with pipe delimiter (|). */


COPY INTO @unload_stage 
from customer_details 
FILE_FORMAT = (FORMAT_NAME = 'pipe_delimited_format' COMPRESSION = NONE);

GET @unload_stage file:///tmp/data/; --update the local DIR path here and run this command from snowsql


